{
  TEST_TERM_DOWMLOD=true;
  MsgPrinter.enableStatusMsgs(true);
  MsgPrinter.enableErrorMsgs(true);
  MsgPrinter.printStatusMsg("Creating tokenizer...");
  if (!OpenNLP.createTokenizer("res/nlp/tokenizer/opennlp/EnglishTok.bin.gz"))   MsgPrinter.printErrorMsg("Could not create tokenizer.");
  MsgPrinter.printStatusMsg("Creating stemmer...");
  SnowballStemmer.create();
  MsgPrinter.printStatusMsg("Creating NE taggers...");
  NETagger.loadListTaggers("res/nlp/netagger/lists/");
  NETagger.loadRegExTaggers("res/nlp/netagger/patterns.lst");
  MsgPrinter.printStatusMsg("  ...loading models");
  MsgPrinter.printStatusMsg("  ...done");
  WikipediaTermImportanceFilter wtif=new WikipediaTermImportanceFilter(NO_NORMALIZATION,NO_NORMALIZATION,false);
  TRECTarget[] targets=TREC13To16Parser.loadTargets(args[0]);
  for (  TRECTarget target : targets) {
    String question=target.getTargetDesc();
    MsgPrinter.printGeneratingQueries();
    String qn=QuestionNormalizer.normalize(question);
    MsgPrinter.printNormalization(qn);
    Logger.logNormalization(qn);
    String[] kws=KeywordExtractor.getKeywords(qn);
    AnalyzedQuestion aq=new AnalyzedQuestion(question);
    aq.setKeywords(kws);
    aq.setFactoid(false);
    Query[] queries=new BagOfWordsG().generateQueries(aq);
    for (int q=0; q < queries.length; q++)     queries[q].setOriginalQueryString(question);
    Result[] results=new Result[1];
    results[0]=new Result("This would be the answer",queries[0]);
    wtif.apply(results);
  }
}
