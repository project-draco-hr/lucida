{
  TEST_TARGET_GENERATION=true;
  MsgPrinter.enableStatusMsgs(true);
  MsgPrinter.enableErrorMsgs(true);
  MsgPrinter.printStatusMsg("Creating tokenizer...");
  if (!OpenNLP.createTokenizer("res/nlp/tokenizer/opennlp/EnglishTok.bin.gz"))   MsgPrinter.printErrorMsg("Could not create tokenizer.");
  MsgPrinter.printStatusMsg("Creating stemmer...");
  SnowballStemmer.create();
  MsgPrinter.printStatusMsg("Creating POS tagger...");
  if (!OpenNLP.createPosTagger("res/nlp/postagger/opennlp/tag.bin.gz","res/nlp/postagger/opennlp/tagdict"))   MsgPrinter.printErrorMsg("Could not create OpenNLP POS tagger.");
  MsgPrinter.printStatusMsg("Creating chunker...");
  if (!OpenNLP.createChunker("res/nlp/phrasechunker/opennlp/" + "EnglishChunk.bin.gz"))   MsgPrinter.printErrorMsg("Could not create chunker.");
  MsgPrinter.printStatusMsg("Creating NE taggers...");
  NETagger.loadListTaggers("res/nlp/netagger/lists/");
  NETagger.loadRegExTaggers("res/nlp/netagger/patterns.lst");
  MsgPrinter.printStatusMsg("  ...loading models");
  if (!StanfordNeTagger.isInitialized() && !StanfordNeTagger.init())   MsgPrinter.printErrorMsg("Could not create Stanford NE tagger.");
  MsgPrinter.printStatusMsg("  ...done");
  WebTermImportanceFilter wtif=new TargetGeneratorTest(NO_NORMALIZATION);
  TRECTarget[] targets=TREC13To16Parser.loadTargets(args[0]);
  for (  TRECTarget target : targets) {
    String question=target.getTargetDesc();
    MsgPrinter.printGeneratingQueries();
    String qn=QuestionNormalizer.normalize(question);
    MsgPrinter.printNormalization(qn);
    Logger.logNormalization(qn);
    String[] kws=KeywordExtractor.getKeywords(qn);
    AnalyzedQuestion aq=new AnalyzedQuestion(question);
    aq.setKeywords(kws);
    aq.setFactoid(false);
    Query[] queries=new BagOfWordsG().generateQueries(aq);
    for (int q=0; q < queries.length; q++)     queries[q].setOriginalQueryString(question);
    Result[] results=new Result[1];
    results[0]=new Result("This would be the answer",queries[0]);
    wtif.apply(results);
  }
}
